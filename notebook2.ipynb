{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0            1               2           3               4   \\\n",
      "0  marketplace  customer_id       review_id  product_id  product_parent   \n",
      "1           US     17747349  R2EI7QLPK4LF7U  B00U7LCE6A       106182406   \n",
      "2           US     10956619  R1W5OMFK1Q3I3O  B00HRJMOM4       162269768   \n",
      "3           US     13132245   RPZWSYWRP92GI  B00P31G9PQ       831433899   \n",
      "4           US     35717248  R2WQWM04XHD9US  B00FGDEPDY       991059534   \n",
      "\n",
      "                                           5                 6            7   \\\n",
      "0                               product_title  product_category  star_rating   \n",
      "1                    CCleaner Free [Download]  Digital_Software            4   \n",
      "2          ResumeMaker Professional Deluxe 18  Digital_Software            3   \n",
      "3                   Amazon Drive Desktop [PC]  Digital_Software            1   \n",
      "4  Norton Internet Security 1 User 3 Licenses  Digital_Software            5   \n",
      "\n",
      "              8            9     10                 11                  12  \\\n",
      "0  helpful_votes  total_votes  vine  verified_purchase     review_headline   \n",
      "1              0            0     N                  Y          Four Stars   \n",
      "2              0            0     N                  Y         Three Stars   \n",
      "3              1            2     N                  Y            One Star   \n",
      "4              0            0     N                  Y  Works as Expected!   \n",
      "\n",
      "                              13           14  \n",
      "0                    review_body  review_date  \n",
      "1                 So far so good   2015-08-31  \n",
      "2  Needs a little more work.....   2015-08-31  \n",
      "3                 Please cancel.   2015-08-31  \n",
      "4             Works as Expected!   2015-08-31  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frederico\\AppData\\Local\\Temp\\ipykernel_9728\\643836554.py:6: DtypeWarning: Columns (1,4,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('amazon_reviews_us_Digital_Software_v1_00.tsv', sep='\\t', header=None, on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# dataset is accessible at https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt (https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_02.tsv.gz)\n",
    "df = pd.read_csv('amazon_reviews_us_Digital_Software_v1_00.tsv', sep='\\t', header=None, on_bad_lines='skip')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101837 entries, 0 to 101836\n",
      "Data columns (total 15 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   0       101837 non-null  object\n",
      " 1   1       101837 non-null  object\n",
      " 2   2       101837 non-null  object\n",
      " 3   3       101837 non-null  object\n",
      " 4   4       101837 non-null  object\n",
      " 5   5       101837 non-null  object\n",
      " 6   6       101837 non-null  object\n",
      " 7   7       101837 non-null  object\n",
      " 8   8       101837 non-null  object\n",
      " 9   9       101837 non-null  object\n",
      " 10  10      101837 non-null  object\n",
      " 11  11      101837 non-null  object\n",
      " 12  12      101837 non-null  object\n",
      " 13  13      101837 non-null  object\n",
      " 14  14      101832 non-null  object\n",
      "dtypes: object(15)\n",
      "memory usage: 11.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.shape\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   marketplace  customer_id       review_id  product_id  product_parent  \\\n",
      "0  marketplace  customer_id       review_id  product_id  product_parent   \n",
      "1           US     17747349  R2EI7QLPK4LF7U  B00U7LCE6A       106182406   \n",
      "2           US     10956619  R1W5OMFK1Q3I3O  B00HRJMOM4       162269768   \n",
      "3           US     13132245   RPZWSYWRP92GI  B00P31G9PQ       831433899   \n",
      "4           US     35717248  R2WQWM04XHD9US  B00FGDEPDY       991059534   \n",
      "\n",
      "                                product_title  product_category  star_rating  \\\n",
      "0                               product_title  product_category  star_rating   \n",
      "1                    CCleaner Free [Download]  Digital_Software            4   \n",
      "2          ResumeMaker Professional Deluxe 18  Digital_Software            3   \n",
      "3                   Amazon Drive Desktop [PC]  Digital_Software            1   \n",
      "4  Norton Internet Security 1 User 3 Licenses  Digital_Software            5   \n",
      "\n",
      "   helpful_votes  total_votes  vine  verified_purchase     review_headline  \\\n",
      "0  helpful_votes  total_votes  vine  verified_purchase     review_headline   \n",
      "1              0            0     N                  Y          Four Stars   \n",
      "2              0            0     N                  Y         Three Stars   \n",
      "3              1            2     N                  Y            One Star   \n",
      "4              0            0     N                  Y  Works as Expected!   \n",
      "\n",
      "                     review_body  review_date  \n",
      "0                    review_body  review_date  \n",
      "1                 So far so good   2015-08-31  \n",
      "2  Needs a little more work.....   2015-08-31  \n",
      "3                 Please cancel.   2015-08-31  \n",
      "4             Works as Expected!   2015-08-31  \n"
     ]
    }
   ],
   "source": [
    "df.columns = ['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category', 'star_rating', \n",
    "            'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date']\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                                       So far so good\n",
      "2                        Needs a little more work.....\n",
      "3                                       Please cancel.\n",
      "4                                   Works as Expected!\n",
      "5    I've had Webroot for a few years. It expired a...\n",
      "6    EXCELLENT software !!!!!  Don't need to do any...\n",
      "7    The variations created by Win10 induced this p...\n",
      "8    Horrible!  Would not upgrade previous version ...\n",
      "9                                      Waste of time .\n",
      "Name: review_body, dtype: object\n",
      "1    4\n",
      "2    3\n",
      "3    1\n",
      "4    5\n",
      "5    4\n",
      "6    5\n",
      "7    1\n",
      "8    1\n",
      "9    1\n",
      "Name: star_rating, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['review_body'][1:10])\n",
    "print(df['star_rating'][1:10])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split \n",
    "\n",
    "# train, test = train_test_split(df, test_size=0.30, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = df['review_body'][1:]\n",
    "all_ratings = df['star_rating'][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So far so good', 'Needs a little more work.....', 'Please cancel.', 'Works as Expected!', \"I've had Webroot for a few years. It expired and I decided to purchase a renewal on Amazon. I went through hell trying to uninstall the expired version in order to install the new.  I called Webroot and had their representative remote into my computer at his request. He was clueless as a bad joke and consumed 29 minutes and 57 seconds of my time forever.  He initially told me it wasn't compatible with Windows 10, but I finally managed to convince him that it is indeed compatible with Windows 10 as it was working on my computer before it expired and also I showed him a review on Amazon to convince him that it works on Windows 10. Finally, he offered to connect me with a senior consultant for over 100 dollars. I declined and told him I'd fix the issue myself. This guy was less helpful than a severed limb.  After spending some time on Google, the issue is now fixed. Webroot should just get rid of their customer service and pay Google for indexing much more helpful info that their dedicated customer service can offer.  As far as the software itself. I think it scans fast, does not slow down my computer and I hope (like most other people including experts) that it's very effective in removing and preventing malware.  Years ago I did extensive research and found it to be among the best, but that was over 4 years ago. Things are fluid in the malware kingdom.  To those experiencing the same issue installing as I did - My advice - don't bother uninstalling the old version, rather launch webroot and click on  &#34;my account&#34; on the right side, then copy and past your product key in the area that says &#34;enter a new keycode&#34;, then click on Activate. This will save numerous painful hours of trying to get the thing to work.\", \"EXCELLENT software !!!!!  Don't need to do anything when it's set up in automatic mode. Noticed a big difference after the first scan/tests\", \"The variations created by Win10 induced this program to eat every bit of work up to that time. All of it, hundreds of hours of work. Guess who's unhappy.\", \"Horrible!  Would not upgrade previous version files and when I gave up on that and figured I'd just move forward it wouldn't let me log in, even after numerous creations of a new account, trying an existing account.  Very frustrating.  I returned it.\", 'Waste of time .', 'Work as easy as other tools that I have used.']\n"
     ]
    }
   ],
   "source": [
    "# reviews_train = list()\n",
    "# for review in train['review_body']:\n",
    "#     reviews_train.append(review)\n",
    "\n",
    "# for review in test['review_body']:\n",
    "#     reviews_test.append(review)\n",
    "\n",
    "# print(reviews_train[:10])\n",
    "# print(reviews_test[:10])\n",
    "\n",
    "reviews = list()\n",
    "\n",
    "for review in all_reviews:\n",
    "    reviews.append(review)\n",
    "\n",
    "print(reviews[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['far good', 'need littl work', 'pleas cancel', 'work expect', 'webroot year expir decid purchas renew amazon went hell tri uninstal expir version order instal new call webroot repres remot comput request clueless bad joke consum minut second time forev initi told compat window final manag convinc inde compat window work comput expir also show review amazon convinc work window final offer connect senior consult dollar declin told fix issu guy less help sever limb spend time googl issu fix webroot get rid custom servic pay googl index much help info dedic custom servic offer far softwar think scan fast slow comput hope like peopl includ expert effect remov prevent malwar year ago extens research found among best year ago thing fluid malwar kingdom experienc issu instal advic bother uninstal old version rather launch webroot click account right side copi past product key area say enter new keycod click activ save numer pain hour tri get thing work', 'excel softwar need anyth set automat mode notic big differ first scan test', 'variat creat win induc program eat everi bit work time hundr hour work guess unhappi', 'horribl would upgrad previou version file gave figur move forward let log even numer creation new account tri exist account frustrat return', 'wast time', 'work easi tool use']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# corpus_train = []\n",
    "# ps = PorterStemmer()\n",
    "# sw = set(stopwords.words('english'))\n",
    "# for i in range(0, len(reviews_train)):\n",
    "#     # get review and remove non alpha chars\n",
    "#     review = re.sub('[^a-zA-Z]', ' ', reviews_train[i])\n",
    "#     # to lower-case\n",
    "#     review = review.lower()\n",
    "#     # split into tokens, apply stemming and remove stop words\n",
    "#     review = ' '.join([ps.stem(w) for w in review.split() if w not in sw])\n",
    "#     corpus_train.append(review)\n",
    "\n",
    "# corpus_test = []\n",
    "# for i in range(0, len(reviews_test)):\n",
    "#     # get review and remove non alpha chars\n",
    "#     review = re.sub('[^a-zA-Z]', ' ', reviews_test[i])\n",
    "#     # to lower-case\n",
    "#     review = review.lower()\n",
    "#     # split into tokens, apply stemming and remove stop words\n",
    "#     review = ' '.join([ps.stem(w) for w in review.split() if w not in sw])\n",
    "#     corpus_test.append(review)   \n",
    "\n",
    "# print(corpus_train[:10])\n",
    "# print(corpus_test[:10])\n",
    "\n",
    "corpus = list()\n",
    "ps = PorterStemmer()\n",
    "sw = set(stopwords.words('english'))\n",
    "for i in range(0, len(reviews)):\n",
    "    # get review and remove non alpha chars\n",
    "    review = re.sub('[^a-zA-Z]', ' ', reviews[i])\n",
    "    # to lower-case\n",
    "    review = review.lower()\n",
    "    # split into tokens, apply stemming and remove stop words\n",
    "    review = ' '.join([ps.stem(w) for w in review.split() if w not in sw])\n",
    "    corpus.append(review)\n",
    "\n",
    "print(corpus[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     4\n",
      "2     3\n",
      "3     1\n",
      "4     5\n",
      "5     4\n",
      "6     5\n",
      "7     1\n",
      "8     1\n",
      "9     1\n",
      "10    5\n",
      "Name: star_rating, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# ratings_train = list()\n",
    "# ratings_test = list()\n",
    "# for rating in train['star_rating']:\n",
    "#     ratings_train.append(rating)\n",
    "\n",
    "# for rating in test['star_rating']:\n",
    "#     ratings_test.append(rating)\n",
    "\n",
    "# print(ratings_train[:10])\n",
    "# print(ratings_test[:10])\n",
    "\n",
    "#print(all_ratings[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #word embeddings\n",
    "# import gensim\n",
    "# from gensim.models import Word2Vec,KeyedVectors\n",
    "\n",
    "# model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True,limit=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def text_to_vector(embeddings, text, sequence_len):\n",
    "    \n",
    "#     # split text into tokens\n",
    "#     tokens = text.split()\n",
    "    \n",
    "#     # convert tokens to embedding vectors, up to sequence_len tokens\n",
    "#     vec = []\n",
    "#     n = 0\n",
    "#     i = 0\n",
    "#     while i < len(tokens) and n < sequence_len:   # while there are tokens and did not reach desired sequence length\n",
    "#         try:\n",
    "#             vec.extend(embeddings.get_vector(tokens[i]))\n",
    "#             n += 1\n",
    "#         except KeyError:\n",
    "#             True   # simply ignore out-of-vocabulary tokens\n",
    "#         finally:\n",
    "#             i += 1\n",
    "    \n",
    "#     # add blanks up to sequence_len, if needed\n",
    "#     for j in range(sequence_len - n):\n",
    "#         vec.extend(np.zeros(embeddings.vector_size,))\n",
    "    \n",
    "#     return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_corpus = []\n",
    "# for c in corpus:\n",
    "#     embeddings_corpus.append(text_to_vector(model, c, 10))\n",
    "\n",
    "# X = np.array(embeddings_corpus)\n",
    "# y = all_ratings\n",
    "\n",
    "# print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bewar download includ malwar', 'husband yet tell like sorri use seem pretti cool easi use', 'microsoft need fix bug outlook found far probabl sure', 'easi purchas download smooth hesit download never problem nice product within coupl minut purchas nice', 'wast money read comment first would purchas yuck', '', 'use avast long time think good everyon made web experi secur viru malwar worri free strongli recommend softwar experi', 'use tt ten year unfortun year program simpli work use window googl chrome browser troubl download updat thought okay realiz congress wait last minut come back went ahead entir tax return went back tri download updat download stall minut tri sever time alway result tri simpli file return without updat best idea pretti simpl return program allow e file although cours took electron file state return intuit clearli know problem superpatch websit download manual unfortun thing happen tri download superpatch spent hour hold repeatedli attempt download tri contact tubotax unsuccess compani clearli know problem littl address satisfactori seem like tt fine long everyth work glitch program compani nowher found tax file appoint h r block', 'purchas pc matic anoth year use past year great product found staff help would problem issu far kept two comput safe virus also clean daili basi', 'much harder thing quicken compar microsoft money might think lack familiar product compel move quicken year ago microsoft opt leav market one thing anoth quicken paycheck stop propag predefin schedul could categor expens associ split transact back paycheck reason cannot deduc duplic transfer transact paycheck automat deposit save check vacat account set paycheck deposit account replic transact save vacat account overst option would buy']\n",
      "49099    1\n",
      "42174    5\n",
      "86748    3\n",
      "98554    5\n",
      "62820    1\n",
      "4280     5\n",
      "61885    5\n",
      "88289    1\n",
      "6986     5\n",
      "86758    2\n",
      "Name: star_rating, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test = train_test_split(corpus, test_size=0.30, random_state=42)\n",
    "y_train, y_test = train_test_split(all_ratings, test_size=0.30, random_state=42)\n",
    "\n",
    "print(X_train[:10])\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9728\\2418228587.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#create Tensor Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#create Tensor Dataset\n",
    "train_data=TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "test_data=TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "\n",
    "#dataloader\n",
    "batch_size=50\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentalLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):    \n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_size=output_size\n",
    "        self.n_layers=n_layers\n",
    "        self.hidden_dim=hidden_dim\n",
    "        \n",
    "        #Embedding and LSTM layers\n",
    "        self.embedding=nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm=nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        #dropout layer\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "        \n",
    "        #Linear and sigmoid layer\n",
    "        self.fc1=nn.Linear(hidden_dim, 64)\n",
    "        self.fc2=nn.Linear(64, 16)\n",
    "        self.fc3=nn.Linear(16,output_size)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size=x.size()\n",
    "        \n",
    "        #Embadding and LSTM output\n",
    "        embedd=self.embedding(x)\n",
    "        lstm_out, hidden=self.lstm(embedd, hidden)\n",
    "        \n",
    "        #stack up the lstm output\n",
    "        lstm_out=lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        #dropout and fully connected layers\n",
    "        out=self.dropout(lstm_out)\n",
    "        out=self.fc1(out)\n",
    "        out=self.dropout(out)\n",
    "        out=self.fc2(out)\n",
    "        out=self.dropout(out)\n",
    "        out=self.fc3(out)\n",
    "        sig_out=self.sigmoid(out)\n",
    "        \n",
    "        sig_out=sig_out.view(batch_size, -1)\n",
    "        sig_out=sig_out[:, -1]\n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize Hidden STATE\"\"\"\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs=inputs.cuda()\n",
    "            labels=labels.cuda()\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()  \n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
