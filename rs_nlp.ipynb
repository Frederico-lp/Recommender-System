{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# dataset is accessible at https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt (https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_02.tsv.gz)\n",
    "df = pd.read_csv('amazon_reviews_us_Digital_Software_v1_00.tsv', sep='\\t', dtype={'star_rating': float})\n",
    "\n",
    "df.columns = ['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category', 'star_rating',\n",
    "              'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date']\n",
    "\n",
    "df.drop(['marketplace', 'product_parent', 'product_title', 'product_category', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
    "         'review_headline'],\n",
    "        axis='columns', inplace=True)\n",
    "\n",
    "customer_count = df['customer_id'].value_counts()\n",
    "customers_with_multiple_reviews = customer_count[customer_count > 3].index\n",
    "df = df[df['customer_id'].isin(customers_with_multiple_reviews)]\n",
    "\n",
    "product_count = df['product_id'].value_counts()\n",
    "products_with_multiple_reviews = product_count[product_count > 3].index\n",
    "df = df[df['product_id'].isin(products_with_multiple_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "user_final_rating = pickle.load(open('./user_final_rating.pkl','rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('parrot.pkl', 'rb') as f:\n",
    "    vocab_to_int = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim))\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim))\n",
    "        \n",
    "        hidden = (h0,c0)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_layers = 2\n",
    "vocab_size = len(vocab_to_int) + 1 #extra 1 for padding\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# function for loaduing weights of a trained model\n",
    "def load_weights(model, weights_dir):\n",
    "    files = os.listdir(weights_dir)\n",
    "    weight_paths = [os.path.join(weights_dir, basename) for basename in files]\n",
    "    # get the latest file in the directory\n",
    "    final_weight_file = os.path.basename(max(weight_paths, key=os.path.getctime))\n",
    "\n",
    "    # first model needs to be loaded\n",
    "    model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
    "\n",
    "    # fixes odd error when state_dict has prescript \"model.\"\" in keys\n",
    "    state_dict = torch.load(os.path.join(weights_dir, final_weight_file))\n",
    "    for key in list(state_dict.keys()):\n",
    "        if key.startswith(\"model.\"):\n",
    "            state_dict[key[6:]] = state_dict.pop(key)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    print('Loaded weights: ' + final_weight_file)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights: epoch-0_accuracy-0.84.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(28008, 64)\n",
       "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
    "\n",
    "lstm_model = load_weights(lstm_model, 'weights2')\n",
    "lstm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_features(reviews_int, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "        \n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-review_len))\n",
    "            new = zeroes+review\n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    for word in test_words:\n",
    "        if word in vocab_to_int.keys():\n",
    "            test_ints.append(vocab_to_int[word])\n",
    "\n",
    "    #test_ints.append([vocab_to_int[word] for word in test_words])\n",
    "    test_ints = [test_ints] \n",
    "\n",
    "    return test_ints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def lstm_predict(test_review):\n",
    "    \n",
    "    lstm_model.eval()\n",
    "    \n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=250\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = lstm_model.init_hidden(batch_size)\n",
    "    \n",
    "    #feature_tensor = feature_tensor.to(device)\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = lstm_model(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: Terrible\n",
      "Sentiment  : 1.0\n"
     ]
    }
   ],
   "source": [
    "review_text = \"Terrible\"\n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Sentiment  : {lstm_predict(review_text)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "bert_model = BertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(SentimentClassifier,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(p = 0.3)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size,num_classes)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "    def forward(self,input_ids , attention_mask):\n",
    "        temp = self.bert(input_ids,attention_mask) # Here we have added one linear layer on top of \n",
    "        pooled_output = temp[1]                    # BERT-base with number of output = 3 \n",
    "        out = self.dropout(pooled_output)          # \n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "# function for loaduing weights of a trained model\n",
    "def load_weights(model, weights_dir):\n",
    "    files = os.listdir(weights_dir)\n",
    "    weight_paths = [os.path.join(weights_dir, basename) for basename in files]\n",
    "    # get the latest file in the directory\n",
    "    final_weight_file = os.path.basename(max(weight_paths, key=os.path.getctime))\n",
    "\n",
    "    # first model needs to be loaded\n",
    "    model = SentimentClassifier(num_classes)\n",
    "\n",
    "    # fixes odd error when state_dict has prescript \"model.\"\" in keys\n",
    "    state_dict = torch.load(os.path.join(weights_dir, final_weight_file))\n",
    "    for key in list(state_dict.keys()):\n",
    "        if key.startswith(\"model.\"):\n",
    "            state_dict[key[6:]] = state_dict.pop(key)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    print('Loaded weights: ' + final_weight_file)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights: epoch-0_accuracy-3.510.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentimentClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = SentimentClassifier(2)\n",
    "\n",
    "bert_model = load_weights(bert_model, 'weights3')\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def bert_predict(text):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=128,\n",
    "      truncation=True,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt')\n",
    "    output = bert_model(encoding['input_ids'],encoding['attention_mask'])\n",
    "    _,prediction = torch.max(output, dim=1)\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: Terrible, i hated it\n",
      "Sentiment  : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Frederico\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "review_text = \"Terrible, i hated it\"\n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Sentiment  : {bert_predict(review_text)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Recommender system with NLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_get_sentiment_product(product_id):\n",
    "    ## Get review list for given product\n",
    "    product_name_review_list = df.loc[df['product_id'] == product_id]['review_body']\n",
    "    sum = 0\n",
    "    for review in product_name_review_list:\n",
    "        sum += bert_predict(review)\n",
    "    ## Predict sentiment\n",
    "    return sum / len(product_name_review_list)\n",
    "\n",
    "def bert_get_sentiment_products(products):\n",
    "    list = []\n",
    "    if len(products) > 1:\n",
    "        for product in products:\n",
    "            list.append(bert_get_sentiment_product(product))\n",
    "    else:\n",
    "        list.append(bert_get_sentiment_product(products))\n",
    "\n",
    "    return list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_get_sentiment_product(product_id):\n",
    "    ## Get review list for given product\n",
    "    product_name_review_list = df.loc[df['product_id'] == product_id]['review_body']\n",
    "    sum = 0\n",
    "    for review in product_name_review_list:\n",
    "        sum += lstm_predict(review)\n",
    "    ## Predict sentiment\n",
    "    return sum / len(product_name_review_list)\n",
    "\n",
    "def lstm_get_sentiment_products(products):\n",
    "    list = []\n",
    "    if len(products) > 1:\n",
    "        for product in products:\n",
    "            list.append(lstm_get_sentiment_product(product))\n",
    "    else:\n",
    "        list.append(lstm_get_sentiment_product(products))\n",
    "\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def find_top_recommendations(pred_rating_df, userid, topn):\n",
    "    recommendation = pred_rating_df.loc[userid].sort_values(ascending=False)[0:topn]\n",
    "    recommendation = pd.DataFrame(recommendation).reset_index().rename(columns={userid:'predicted_ratings'})\n",
    "    return recommendation\n",
    "\n",
    "\n",
    "def bert_find_top_pos_recommendation(user_final_rating, user_id, no_recommendation):\n",
    "    ## 10 is manually coded, need to change \n",
    "    ## Generate top recommenddations using user-user based recommendation system w/o using sentiment analysis  \n",
    "    recommendation_user_user = find_top_recommendations(user_final_rating, user_id, 10)\n",
    "    recommendation_user_user['userId'] = user_id\n",
    "    ## filter out recommendations where predicted rating is zero\n",
    "    recommendation_user_user = recommendation_user_user[recommendation_user_user['predicted_ratings']!=0]\n",
    "    print(\"Recommended products for user id:{} without using sentiment\".format(user_id))\n",
    "    display(recommendation_user_user)\n",
    "    ## Get overall sentiment score for each recommended product\n",
    "\n",
    "    #recommendation_user_user['sentiment_score'] = recommendation_user_user['product_id'].apply(get_sentiment_product)\n",
    "    sentiments = bert_get_sentiment_products(recommendation_user_user['product_id'])\n",
    "    recommendation_user_user['sentiment_score'] = sentiments\n",
    "    ## Transform scale of sentiment so that it can be manipulated with predicted rating score\n",
    "    scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "    scaler.fit(recommendation_user_user[['sentiment_score']])\n",
    "    recommendation_user_user['sentiment_score'] = scaler.transform(recommendation_user_user[['sentiment_score']])\n",
    "    ## Get final product ranking score using 1*Predicted rating of recommended product + 2*normalized sentiment score on scale of 1–5 of recommended product \n",
    "    recommendation_user_user['product_ranking_score'] =  1*recommendation_user_user['predicted_ratings'] + \\\n",
    "                                                        2*recommendation_user_user['sentiment_score']\n",
    "    print(\"Recommended products for user id:{} after using sentiment\".format(user_id))\n",
    "    ## Sort product ranking score in descending order and show only top `no_recommendation`\n",
    "    display(recommendation_user_user.sort_values(by = ['product_ranking_score'],ascending = False).head(no_recommendation))\n",
    "\n",
    "\n",
    "\n",
    "def lstm_find_top_pos_recommendation(user_final_rating, user_id, no_recommendation):\n",
    "    ## 10 is manually coded, need to change \n",
    "    ## Generate top recommenddations using user-user based recommendation system w/o using sentiment analysis  \n",
    "    recommendation_user_user = find_top_recommendations(user_final_rating, user_id, 10)\n",
    "    recommendation_user_user['userId'] = user_id\n",
    "    ## filter out recommendations where predicted rating is zero\n",
    "    recommendation_user_user = recommendation_user_user[recommendation_user_user['predicted_ratings']!=0]\n",
    "    print(\"Recommended products for user id:{} without using sentiment\".format(user_id))\n",
    "    display(recommendation_user_user)\n",
    "    ## Get overall sentiment score for each recommended product\n",
    "\n",
    "    #recommendation_user_user['sentiment_score'] = recommendation_user_user['product_id'].apply(get_sentiment_product)\n",
    "    sentiments = lstm_get_sentiment_products(recommendation_user_user['product_id'])\n",
    "    recommendation_user_user['sentiment_score'] = sentiments\n",
    "    ## Transform scale of sentiment so that it can be manipulated with predicted rating score\n",
    "    scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "    scaler.fit(recommendation_user_user[['sentiment_score']])\n",
    "    recommendation_user_user['sentiment_score'] = scaler.transform(recommendation_user_user[['sentiment_score']])\n",
    "    ## Get final product ranking score using 1*Predicted rating of recommended product + 2*normalized sentiment score on scale of 1–5 of recommended product \n",
    "    recommendation_user_user['product_ranking_score'] =  1*recommendation_user_user['predicted_ratings'] + \\\n",
    "                                                        2*recommendation_user_user['sentiment_score']\n",
    "    print(\"Recommended products for user id:{} after using sentiment\".format(user_id))\n",
    "    ## Sort product ranking score in descending order and show only top `no_recommendation`\n",
    "    display(recommendation_user_user.sort_values(by = ['product_ranking_score'],ascending = False).head(no_recommendation))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended products for user id:12975480 without using sentiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>predicted_ratings</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B003GAMZFO</td>\n",
       "      <td>3.726780</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00FFIO0NA</td>\n",
       "      <td>3.726780</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B005S4Y65I</td>\n",
       "      <td>3.726780</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B004P1I2GE</td>\n",
       "      <td>3.259259</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00EV6JPRS</td>\n",
       "      <td>2.981424</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B00H9A60O4</td>\n",
       "      <td>2.723607</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B00J04F9LW</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B00FFINOWS</td>\n",
       "      <td>1.490712</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B00FGDEPDY</td>\n",
       "      <td>1.487375</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B0068TJ0XK</td>\n",
       "      <td>0.596285</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  predicted_ratings    userId\n",
       "0  B003GAMZFO           3.726780  12975480\n",
       "1  B00FFIO0NA           3.726780  12975480\n",
       "2  B005S4Y65I           3.726780  12975480\n",
       "3  B004P1I2GE           3.259259  12975480\n",
       "4  B00EV6JPRS           2.981424  12975480\n",
       "5  B00H9A60O4           2.723607  12975480\n",
       "6  B00J04F9LW           2.500000  12975480\n",
       "7  B00FFINOWS           1.490712  12975480\n",
       "8  B00FGDEPDY           1.487375  12975480\n",
       "9  B0068TJ0XK           0.596285  12975480"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Frederico\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended products for user id:12975480 after using sentiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>predicted_ratings</th>\n",
       "      <th>userId</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>product_ranking_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B004P1I2GE</td>\n",
       "      <td>3.259259</td>\n",
       "      <td>12975480</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.259259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B00J04F9LW</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>12975480</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>11.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00FFIO0NA</td>\n",
       "      <td>3.726780</td>\n",
       "      <td>12975480</td>\n",
       "      <td>3.787879</td>\n",
       "      <td>11.302538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B003GAMZFO</td>\n",
       "      <td>3.726780</td>\n",
       "      <td>12975480</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>10.393447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00EV6JPRS</td>\n",
       "      <td>2.981424</td>\n",
       "      <td>12975480</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>9.648091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B005S4Y65I</td>\n",
       "      <td>3.726780</td>\n",
       "      <td>12975480</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>9.282336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B00FFINOWS</td>\n",
       "      <td>1.490712</td>\n",
       "      <td>12975480</td>\n",
       "      <td>3.787879</td>\n",
       "      <td>9.066470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B00H9A60O4</td>\n",
       "      <td>2.723607</td>\n",
       "      <td>12975480</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>8.279162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B00FGDEPDY</td>\n",
       "      <td>1.487375</td>\n",
       "      <td>12975480</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>6.154042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B0068TJ0XK</td>\n",
       "      <td>0.596285</td>\n",
       "      <td>12975480</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.596285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  predicted_ratings    userId  sentiment_score  \\\n",
       "3  B004P1I2GE           3.259259  12975480         5.000000   \n",
       "6  B00J04F9LW           2.500000  12975480         4.444444   \n",
       "1  B00FFIO0NA           3.726780  12975480         3.787879   \n",
       "0  B003GAMZFO           3.726780  12975480         3.333333   \n",
       "4  B00EV6JPRS           2.981424  12975480         3.333333   \n",
       "2  B005S4Y65I           3.726780  12975480         2.777778   \n",
       "7  B00FFINOWS           1.490712  12975480         3.787879   \n",
       "5  B00H9A60O4           2.723607  12975480         2.777778   \n",
       "8  B00FGDEPDY           1.487375  12975480         2.333333   \n",
       "9  B0068TJ0XK           0.596285  12975480         1.000000   \n",
       "\n",
       "   product_ranking_score  \n",
       "3              13.259259  \n",
       "6              11.388889  \n",
       "1              11.302538  \n",
       "0              10.393447  \n",
       "4               9.648091  \n",
       "2               9.282336  \n",
       "7               9.066470  \n",
       "5               8.279162  \n",
       "8               6.154042  \n",
       "9               2.596285  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended products for user id:12975480 without using sentiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>predicted_ratings</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B003GAMZFO</td>\n",
       "      <td>3.726780</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00FFIO0NA</td>\n",
       "      <td>3.726780</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B005S4Y65I</td>\n",
       "      <td>3.726780</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B004P1I2GE</td>\n",
       "      <td>3.259259</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00EV6JPRS</td>\n",
       "      <td>2.981424</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B00H9A60O4</td>\n",
       "      <td>2.723607</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B00J04F9LW</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B00FFINOWS</td>\n",
       "      <td>1.490712</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B00FGDEPDY</td>\n",
       "      <td>1.487375</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B0068TJ0XK</td>\n",
       "      <td>0.596285</td>\n",
       "      <td>12975480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  predicted_ratings    userId\n",
       "0  B003GAMZFO           3.726780  12975480\n",
       "1  B00FFIO0NA           3.726780  12975480\n",
       "2  B005S4Y65I           3.726780  12975480\n",
       "3  B004P1I2GE           3.259259  12975480\n",
       "4  B00EV6JPRS           2.981424  12975480\n",
       "5  B00H9A60O4           2.723607  12975480\n",
       "6  B00J04F9LW           2.500000  12975480\n",
       "7  B00FFINOWS           1.490712  12975480\n",
       "8  B00FGDEPDY           1.487375  12975480\n",
       "9  B0068TJ0XK           0.596285  12975480"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended products for user id:12975480 after using sentiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>predicted_ratings</th>\n",
       "      <th>userId</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>product_ranking_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B003GAMZFO</td>\n",
       "      <td>3.726780</td>\n",
       "      <td>12975480</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.726780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00FFIO0NA</td>\n",
       "      <td>3.726780</td>\n",
       "      <td>12975480</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.726780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B005S4Y65I</td>\n",
       "      <td>3.726780</td>\n",
       "      <td>12975480</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.726780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B004P1I2GE</td>\n",
       "      <td>3.259259</td>\n",
       "      <td>12975480</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.259259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00EV6JPRS</td>\n",
       "      <td>2.981424</td>\n",
       "      <td>12975480</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.981424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B00H9A60O4</td>\n",
       "      <td>2.723607</td>\n",
       "      <td>12975480</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.723607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B0068TJ0XK</td>\n",
       "      <td>0.596285</td>\n",
       "      <td>12975480</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.596285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B00J04F9LW</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>12975480</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>9.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B00FGDEPDY</td>\n",
       "      <td>1.487375</td>\n",
       "      <td>12975480</td>\n",
       "      <td>2.485714</td>\n",
       "      <td>6.458804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B00FFINOWS</td>\n",
       "      <td>1.490712</td>\n",
       "      <td>12975480</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.490712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  predicted_ratings    userId  sentiment_score  \\\n",
       "0  B003GAMZFO           3.726780  12975480         5.000000   \n",
       "1  B00FFIO0NA           3.726780  12975480         5.000000   \n",
       "2  B005S4Y65I           3.726780  12975480         5.000000   \n",
       "3  B004P1I2GE           3.259259  12975480         5.000000   \n",
       "4  B00EV6JPRS           2.981424  12975480         5.000000   \n",
       "5  B00H9A60O4           2.723607  12975480         5.000000   \n",
       "9  B0068TJ0XK           0.596285  12975480         5.000000   \n",
       "6  B00J04F9LW           2.500000  12975480         3.428571   \n",
       "8  B00FGDEPDY           1.487375  12975480         2.485714   \n",
       "7  B00FFINOWS           1.490712  12975480         1.000000   \n",
       "\n",
       "   product_ranking_score  \n",
       "0              13.726780  \n",
       "1              13.726780  \n",
       "2              13.726780  \n",
       "3              13.259259  \n",
       "4              12.981424  \n",
       "5              12.723607  \n",
       "9              10.596285  \n",
       "6               9.357143  \n",
       "8               6.458804  \n",
       "7               3.490712  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enter user id\n",
    "user_id = 12975480\n",
    "bert_find_top_pos_recommendation(user_final_rating, user_id, 10)\n",
    "lstm_find_top_pos_recommendation(user_final_rating, user_id, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
