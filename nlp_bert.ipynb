{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0            1               2           3               4   \\\n",
      "0  marketplace  customer_id       review_id  product_id  product_parent   \n",
      "1           US     17747349  R2EI7QLPK4LF7U  B00U7LCE6A       106182406   \n",
      "2           US     10956619  R1W5OMFK1Q3I3O  B00HRJMOM4       162269768   \n",
      "3           US     13132245   RPZWSYWRP92GI  B00P31G9PQ       831433899   \n",
      "4           US     35717248  R2WQWM04XHD9US  B00FGDEPDY       991059534   \n",
      "\n",
      "                                           5                 6            7   \\\n",
      "0                               product_title  product_category  star_rating   \n",
      "1                    CCleaner Free [Download]  Digital_Software            4   \n",
      "2          ResumeMaker Professional Deluxe 18  Digital_Software            3   \n",
      "3                   Amazon Drive Desktop [PC]  Digital_Software            1   \n",
      "4  Norton Internet Security 1 User 3 Licenses  Digital_Software            5   \n",
      "\n",
      "              8            9     10                 11                  12  \\\n",
      "0  helpful_votes  total_votes  vine  verified_purchase     review_headline   \n",
      "1              0            0     N                  Y          Four Stars   \n",
      "2              0            0     N                  Y         Three Stars   \n",
      "3              1            2     N                  Y            One Star   \n",
      "4              0            0     N                  Y  Works as Expected!   \n",
      "\n",
      "                              13           14  \n",
      "0                    review_body  review_date  \n",
      "1                 So far so good   2015-08-31  \n",
      "2  Needs a little more work.....   2015-08-31  \n",
      "3                 Please cancel.   2015-08-31  \n",
      "4             Works as Expected!   2015-08-31  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frederico\\AppData\\Local\\Temp\\ipykernel_9488\\643836554.py:6: DtypeWarning: Columns (1,4,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('amazon_reviews_us_Digital_Software_v1_00.tsv', sep='\\t', header=None, on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# dataset is accessible at https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt (https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_02.tsv.gz)\n",
    "df = pd.read_csv('amazon_reviews_us_Digital_Software_v1_00.tsv', sep='\\t', header=None, on_bad_lines='skip')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101837 entries, 0 to 101836\n",
      "Data columns (total 15 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   0       101837 non-null  object\n",
      " 1   1       101837 non-null  object\n",
      " 2   2       101837 non-null  object\n",
      " 3   3       101837 non-null  object\n",
      " 4   4       101837 non-null  object\n",
      " 5   5       101837 non-null  object\n",
      " 6   6       101837 non-null  object\n",
      " 7   7       101837 non-null  object\n",
      " 8   8       101837 non-null  object\n",
      " 9   9       101837 non-null  object\n",
      " 10  10      101837 non-null  object\n",
      " 11  11      101837 non-null  object\n",
      " 12  12      101837 non-null  object\n",
      " 13  13      101837 non-null  object\n",
      " 14  14      101832 non-null  object\n",
      "dtypes: object(15)\n",
      "memory usage: 11.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.shape\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   marketplace  customer_id       review_id  product_id  product_parent  \\\n",
      "0  marketplace  customer_id       review_id  product_id  product_parent   \n",
      "1           US     17747349  R2EI7QLPK4LF7U  B00U7LCE6A       106182406   \n",
      "2           US     10956619  R1W5OMFK1Q3I3O  B00HRJMOM4       162269768   \n",
      "3           US     13132245   RPZWSYWRP92GI  B00P31G9PQ       831433899   \n",
      "4           US     35717248  R2WQWM04XHD9US  B00FGDEPDY       991059534   \n",
      "\n",
      "                                product_title  product_category  star_rating  \\\n",
      "0                               product_title  product_category  star_rating   \n",
      "1                    CCleaner Free [Download]  Digital_Software            4   \n",
      "2          ResumeMaker Professional Deluxe 18  Digital_Software            3   \n",
      "3                   Amazon Drive Desktop [PC]  Digital_Software            1   \n",
      "4  Norton Internet Security 1 User 3 Licenses  Digital_Software            5   \n",
      "\n",
      "   helpful_votes  total_votes  vine  verified_purchase     review_headline  \\\n",
      "0  helpful_votes  total_votes  vine  verified_purchase     review_headline   \n",
      "1              0            0     N                  Y          Four Stars   \n",
      "2              0            0     N                  Y         Three Stars   \n",
      "3              1            2     N                  Y            One Star   \n",
      "4              0            0     N                  Y  Works as Expected!   \n",
      "\n",
      "                     review_body  review_date  \n",
      "0                    review_body  review_date  \n",
      "1                 So far so good   2015-08-31  \n",
      "2  Needs a little more work.....   2015-08-31  \n",
      "3                 Please cancel.   2015-08-31  \n",
      "4             Works as Expected!   2015-08-31  \n"
     ]
    }
   ],
   "source": [
    "df.columns = ['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category', 'star_rating', \n",
    "            'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date']\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                                       So far so good\n",
      "2                        Needs a little more work.....\n",
      "3                                       Please cancel.\n",
      "4                                   Works as Expected!\n",
      "5    I've had Webroot for a few years. It expired a...\n",
      "6    EXCELLENT software !!!!!  Don't need to do any...\n",
      "7    The variations created by Win10 induced this p...\n",
      "8    Horrible!  Would not upgrade previous version ...\n",
      "9                                      Waste of time .\n",
      "Name: review_body, dtype: object\n",
      "1    4\n",
      "2    3\n",
      "3    1\n",
      "4    5\n",
      "5    4\n",
      "6    5\n",
      "7    1\n",
      "8    1\n",
      "9    1\n",
      "Name: star_rating, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['review_body'][1:10])\n",
    "print(df['star_rating'][1:10])  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = df['review_body'][1:]\n",
    "all_ratings = df['star_rating'][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So far so good', 'Needs a little more work.....', 'Please cancel.', 'Works as Expected!', \"I've had Webroot for a few years. It expired and I decided to purchase a renewal on Amazon. I went through hell trying to uninstall the expired version in order to install the new.  I called Webroot and had their representative remote into my computer at his request. He was clueless as a bad joke and consumed 29 minutes and 57 seconds of my time forever.  He initially told me it wasn't compatible with Windows 10, but I finally managed to convince him that it is indeed compatible with Windows 10 as it was working on my computer before it expired and also I showed him a review on Amazon to convince him that it works on Windows 10. Finally, he offered to connect me with a senior consultant for over 100 dollars. I declined and told him I'd fix the issue myself. This guy was less helpful than a severed limb.  After spending some time on Google, the issue is now fixed. Webroot should just get rid of their customer service and pay Google for indexing much more helpful info that their dedicated customer service can offer.  As far as the software itself. I think it scans fast, does not slow down my computer and I hope (like most other people including experts) that it's very effective in removing and preventing malware.  Years ago I did extensive research and found it to be among the best, but that was over 4 years ago. Things are fluid in the malware kingdom.  To those experiencing the same issue installing as I did - My advice - don't bother uninstalling the old version, rather launch webroot and click on  &#34;my account&#34; on the right side, then copy and past your product key in the area that says &#34;enter a new keycode&#34;, then click on Activate. This will save numerous painful hours of trying to get the thing to work.\", \"EXCELLENT software !!!!!  Don't need to do anything when it's set up in automatic mode. Noticed a big difference after the first scan/tests\", \"The variations created by Win10 induced this program to eat every bit of work up to that time. All of it, hundreds of hours of work. Guess who's unhappy.\", \"Horrible!  Would not upgrade previous version files and when I gave up on that and figured I'd just move forward it wouldn't let me log in, even after numerous creations of a new account, trying an existing account.  Very frustrating.  I returned it.\", 'Waste of time .', 'Work as easy as other tools that I have used.']\n"
     ]
    }
   ],
   "source": [
    "reviews = list()\n",
    "\n",
    "for review in all_reviews:\n",
    "    reviews.append(review)\n",
    "\n",
    "print(reviews[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['far good', 'need littl work', 'pleas cancel', 'work expect', 'webroot year expir decid purchas renew amazon went hell tri uninstal expir version order instal new call webroot repres remot comput request clueless bad joke consum minut second time forev initi told compat window final manag convinc inde compat window work comput expir also show review amazon convinc work window final offer connect senior consult dollar declin told fix issu guy less help sever limb spend time googl issu fix webroot get rid custom servic pay googl index much help info dedic custom servic offer far softwar think scan fast slow comput hope like peopl includ expert effect remov prevent malwar year ago extens research found among best year ago thing fluid malwar kingdom experienc issu instal advic bother uninstal old version rather launch webroot click account right side copi past product key area say enter new keycod click activ save numer pain hour tri get thing work', 'excel softwar need anyth set automat mode notic big differ first scan test', 'variat creat win induc program eat everi bit work time hundr hour work guess unhappi', 'horribl would upgrad previou version file gave figur move forward let log even numer creation new account tri exist account frustrat return', 'wast time', 'work easi tool use']\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# corpus = list()\n",
    "# ps = PorterStemmer()\n",
    "# sw = set(stopwords.words('english'))\n",
    "# for i in range(0, len(reviews)):\n",
    "#     # get review and remove non alpha chars\n",
    "#     review = re.sub('[^a-zA-Z]', ' ', reviews[i])\n",
    "#     # to lower-case\n",
    "#     review = review.lower()\n",
    "#     # split into tokens, apply stemming and remove stop words\n",
    "#     review = ' '.join([ps.stem(w) for w in review.split() if w not in sw])\n",
    "#     corpus.append(review)\n",
    "\n",
    "# print(corpus[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['far', 'good', 'need', 'littl', 'work', 'pleas', 'cancel', 'work', 'expect', 'webroot']\n"
     ]
    }
   ],
   "source": [
    "# all_text = \" \".join(corpus)\n",
    "# all_words = all_text.split()\n",
    "\n",
    "# print(all_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "originally gave this a 2 star but I think it only deserves 1. I simply can't wrap my head around the following. picture this, you add a task to your list. the reminder feature is on.you want to change the time on the reminder feature so you click it.\n",
      "============================================================\n",
      "['originally', 'gave', 'this', 'a', '2', 'star', 'but', 'I', 'think', 'it', 'only', 'deserves', '1', '.', 'I', 'simply', 'can', \"'\", 't', 'wrap', 'my', 'head', 'around', 'the', 'following', '.', 'picture', 'this', ',', 'you', 'add', 'a', 'task', 'to', 'your', 'list', '.', 'the', 'reminder', 'feature', 'is', 'on', '.', 'you', 'want', 'to', 'change', 'the', 'time', 'on', 'the', 'reminder', 'feature', 'so', 'you', 'click', 'it', '.']\n",
      "============================================================\n",
      "[2034, 1522, 1142, 170, 123, 2851, 1133, 146, 1341, 1122, 1178, 18641, 122, 119, 146, 2566, 1169, 112, 189, 10561, 1139, 1246, 1213, 1103, 1378, 119, 3439, 1142, 117, 1128, 5194, 170, 4579, 1106, 1240, 2190, 119, 1103, 15656, 2672, 1110, 1113, 119, 1128, 1328, 1106, 1849, 1103, 1159, 1113, 1103, 15656, 2672, 1177, 1128, 13440, 1122, 119]\n"
     ]
    }
   ],
   "source": [
    "#from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Set the model name\n",
    "MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "# Build a BERT based tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "sample_text = \"originally gave this a 2 star but I think it only deserves 1. I simply can't wrap my head around the following. picture this, you add a task to your list. the reminder feature is on.you want to change the time on the reminder feature so you click it.\"\n",
    "               \n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f'{sample_text}')\n",
    "print('='*60)\n",
    "print(tokens)\n",
    "print('='*60)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Frederico\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "      sample_text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=128,\n",
    "      truncation=True,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train , val and test data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_df , test_df  = train_test_split(reviews, test_size = 0.2, shuffle = True)\n",
    "# val_df , test_df  = train_test_split(test_df, test_size = 0.5, shuffle = True)\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(reviews, all_ratings, train_size=0.8)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9488\\678860666.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# test_df = np.array(test_df)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# train_df = np.array(train_df)\n",
    "# val_df = np.array(val_df)\n",
    "# test_df = np.array(test_df)\n",
    "\n",
    "# print(X_train.shape , X_valid.shape , X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tf\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AmazonReview(Dataset):\n",
    "    def __init__(self, review, target, tokenizer, max_len):\n",
    "        self.review = review\n",
    "        self.target = target\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.review)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        review = self.review[index]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "                          review,\n",
    "                          add_special_tokens=True,\n",
    "                          max_length=128,\n",
    "                          truncation=True,\n",
    "                          return_token_type_ids=False,\n",
    "                          pad_to_max_length=True,\n",
    "                          return_attention_mask=True,\n",
    "                          return_tensors='pt')\n",
    "        \n",
    "        return {'review' : review,\n",
    "                'input_id': encoding['input_ids'].flatten(),\n",
    "                'attention_mask':encoding['attention_mask'].flatten(),\n",
    "                'target': tf.tensor(self.target[index], dtype = tf.long)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "MAX_LEN = 128\n",
    "\n",
    "training_data = AmazonReview(review = np.array(X_train),\n",
    "                               target = np.array(y_train),\n",
    "                               tokenizer = tokenizer,\n",
    "                               max_len = MAX_LEN)\n",
    "\n",
    "validation_data = AmazonReview(review = np.array(X_valid),\n",
    "                               target = np.array(y_valid),\n",
    "                               tokenizer = tokenizer,\n",
    "                               max_len = MAX_LEN)\n",
    "\n",
    "test_data = AmazonReview(review = np.array(X_test),\n",
    "                               target = np.array(y_test),\n",
    "                               tokenizer = tokenizer,\n",
    "                               max_len = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81468 10184 10184\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data),len(test_data),len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "## DataLoader\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(training_data , batch_size = BATCH_SIZE , shuffle = True)\n",
    "test_loader = DataLoader(test_data , batch_size = BATCH_SIZE , shuffle = False)\n",
    "val_loader = DataLoader(validation_data , batch_size = BATCH_SIZE , shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Frederico\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9488\\3951022324.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Frederico\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Frederico\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Frederico\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Frederico\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9488\\4098838825.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;34m'input_id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[1;34m'target'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                }\n",
      "\u001b[1;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(train_loader))\n",
    "sample_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "## testing the output of bert model\n",
    "x = bert_model(sample_batch['input_id'].view(BATCH_SIZE,MAX_LEN)\n",
    "               ,sample_batch['attention_mask'].view(BATCH_SIZE,MAX_LEN))\n",
    "\n",
    "print('Last hidden layer size for input as batch',x[0].shape)\n",
    "print('Pooled output size for input as batch',x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(SentimentClassifier,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(p = 0.3)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size,num_classes)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "    def forward(self,input_ids , attention_mask):\n",
    "        temp = self.bert(input_ids,attention_mask) # Here we have added one linear layer on top of \n",
    "        pooled_output = temp[1]                    # BERT-base with number of output = 3 \n",
    "        out = self.dropout(pooled_output)          # \n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
