{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0            1               2           3               4   \\\n",
      "0  marketplace  customer_id       review_id  product_id  product_parent   \n",
      "1           US     17747349  R2EI7QLPK4LF7U  B00U7LCE6A       106182406   \n",
      "2           US     10956619  R1W5OMFK1Q3I3O  B00HRJMOM4       162269768   \n",
      "3           US     13132245   RPZWSYWRP92GI  B00P31G9PQ       831433899   \n",
      "4           US     35717248  R2WQWM04XHD9US  B00FGDEPDY       991059534   \n",
      "\n",
      "                                           5                 6            7   \\\n",
      "0                               product_title  product_category  star_rating   \n",
      "1                    CCleaner Free [Download]  Digital_Software            4   \n",
      "2          ResumeMaker Professional Deluxe 18  Digital_Software            3   \n",
      "3                   Amazon Drive Desktop [PC]  Digital_Software            1   \n",
      "4  Norton Internet Security 1 User 3 Licenses  Digital_Software            5   \n",
      "\n",
      "              8            9     10                 11                  12  \\\n",
      "0  helpful_votes  total_votes  vine  verified_purchase     review_headline   \n",
      "1              0            0     N                  Y          Four Stars   \n",
      "2              0            0     N                  Y         Three Stars   \n",
      "3              1            2     N                  Y            One Star   \n",
      "4              0            0     N                  Y  Works as Expected!   \n",
      "\n",
      "                              13           14  \n",
      "0                    review_body  review_date  \n",
      "1                 So far so good   2015-08-31  \n",
      "2  Needs a little more work.....   2015-08-31  \n",
      "3                 Please cancel.   2015-08-31  \n",
      "4             Works as Expected!   2015-08-31  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Frederico\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3397: DtypeWarning: Columns (1,4,7,8,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# dataset is accessible at https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt (https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_02.tsv.gz)\n",
    "df = pd.read_csv('amazon_reviews_us_Digital_Software_v1_00.tsv', sep='\\t', header=None, on_bad_lines='skip')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101837 entries, 0 to 101836\n",
      "Data columns (total 15 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   0       101837 non-null  object\n",
      " 1   1       101837 non-null  object\n",
      " 2   2       101837 non-null  object\n",
      " 3   3       101837 non-null  object\n",
      " 4   4       101837 non-null  object\n",
      " 5   5       101837 non-null  object\n",
      " 6   6       101837 non-null  object\n",
      " 7   7       101837 non-null  object\n",
      " 8   8       101837 non-null  object\n",
      " 9   9       101837 non-null  object\n",
      " 10  10      101837 non-null  object\n",
      " 11  11      101837 non-null  object\n",
      " 12  12      101837 non-null  object\n",
      " 13  13      101837 non-null  object\n",
      " 14  14      101832 non-null  object\n",
      "dtypes: object(15)\n",
      "memory usage: 11.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.shape\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   marketplace  customer_id       review_id  product_id  product_parent  \\\n",
      "0  marketplace  customer_id       review_id  product_id  product_parent   \n",
      "1           US     17747349  R2EI7QLPK4LF7U  B00U7LCE6A       106182406   \n",
      "2           US     10956619  R1W5OMFK1Q3I3O  B00HRJMOM4       162269768   \n",
      "3           US     13132245   RPZWSYWRP92GI  B00P31G9PQ       831433899   \n",
      "4           US     35717248  R2WQWM04XHD9US  B00FGDEPDY       991059534   \n",
      "\n",
      "                                product_title  product_category  star_rating  \\\n",
      "0                               product_title  product_category  star_rating   \n",
      "1                    CCleaner Free [Download]  Digital_Software            4   \n",
      "2          ResumeMaker Professional Deluxe 18  Digital_Software            3   \n",
      "3                   Amazon Drive Desktop [PC]  Digital_Software            1   \n",
      "4  Norton Internet Security 1 User 3 Licenses  Digital_Software            5   \n",
      "\n",
      "   helpful_votes  total_votes  vine  verified_purchase     review_headline  \\\n",
      "0  helpful_votes  total_votes  vine  verified_purchase     review_headline   \n",
      "1              0            0     N                  Y          Four Stars   \n",
      "2              0            0     N                  Y         Three Stars   \n",
      "3              1            2     N                  Y            One Star   \n",
      "4              0            0     N                  Y  Works as Expected!   \n",
      "\n",
      "                     review_body  review_date  \n",
      "0                    review_body  review_date  \n",
      "1                 So far so good   2015-08-31  \n",
      "2  Needs a little more work.....   2015-08-31  \n",
      "3                 Please cancel.   2015-08-31  \n",
      "4             Works as Expected!   2015-08-31  \n"
     ]
    }
   ],
   "source": [
    "df.columns = ['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category', 'star_rating', \n",
    "            'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date']\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                                       So far so good\n",
      "2                        Needs a little more work.....\n",
      "3                                       Please cancel.\n",
      "4                                   Works as Expected!\n",
      "5    I've had Webroot for a few years. It expired a...\n",
      "6    EXCELLENT software !!!!!  Don't need to do any...\n",
      "7    The variations created by Win10 induced this p...\n",
      "8    Horrible!  Would not upgrade previous version ...\n",
      "9                                      Waste of time .\n",
      "Name: review_body, dtype: object\n",
      "1    4\n",
      "2    3\n",
      "3    1\n",
      "4    5\n",
      "5    4\n",
      "6    5\n",
      "7    1\n",
      "8    1\n",
      "9    1\n",
      "Name: star_rating, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['review_body'][1:10])\n",
    "print(df['star_rating'][1:10])  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = df['review_body'][1:]\n",
    "all_ratings = df['star_rating'][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = all_reviews[:5000]\n",
    "all_ratings = all_ratings[:5000]\n",
    "all_ratings = all_ratings.to_list()\n",
    "all_ratings = [int(i) for i in all_ratings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(all_ratings):\n",
    "    if x >= 3: all_ratings[i] = 1\n",
    "    else: all_ratings[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(all_ratings[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So far so good', 'Needs a little more work.....', 'Please cancel.', 'Works as Expected!', \"I've had Webroot for a few years. It expired and I decided to purchase a renewal on Amazon. I went through hell trying to uninstall the expired version in order to install the new.  I called Webroot and had their representative remote into my computer at his request. He was clueless as a bad joke and consumed 29 minutes and 57 seconds of my time forever.  He initially told me it wasn't compatible with Windows 10, but I finally managed to convince him that it is indeed compatible with Windows 10 as it was working on my computer before it expired and also I showed him a review on Amazon to convince him that it works on Windows 10. Finally, he offered to connect me with a senior consultant for over 100 dollars. I declined and told him I'd fix the issue myself. This guy was less helpful than a severed limb.  After spending some time on Google, the issue is now fixed. Webroot should just get rid of their customer service and pay Google for indexing much more helpful info that their dedicated customer service can offer.  As far as the software itself. I think it scans fast, does not slow down my computer and I hope (like most other people including experts) that it's very effective in removing and preventing malware.  Years ago I did extensive research and found it to be among the best, but that was over 4 years ago. Things are fluid in the malware kingdom.  To those experiencing the same issue installing as I did - My advice - don't bother uninstalling the old version, rather launch webroot and click on  &#34;my account&#34; on the right side, then copy and past your product key in the area that says &#34;enter a new keycode&#34;, then click on Activate. This will save numerous painful hours of trying to get the thing to work.\", \"EXCELLENT software !!!!!  Don't need to do anything when it's set up in automatic mode. Noticed a big difference after the first scan/tests\", \"The variations created by Win10 induced this program to eat every bit of work up to that time. All of it, hundreds of hours of work. Guess who's unhappy.\", \"Horrible!  Would not upgrade previous version files and when I gave up on that and figured I'd just move forward it wouldn't let me log in, even after numerous creations of a new account, trying an existing account.  Very frustrating.  I returned it.\", 'Waste of time .', 'Work as easy as other tools that I have used.']\n"
     ]
    }
   ],
   "source": [
    "reviews = list()\n",
    "\n",
    "for review in all_reviews:\n",
    "    reviews.append(review)\n",
    "\n",
    "print(reviews[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "originally gave this a 2 star but I think it only deserves 1. I simply can't wrap my head around the following. picture this, you add a task to your list. the reminder feature is on.you want to change the time on the reminder feature so you click it.\n",
      "============================================================\n",
      "['originally', 'gave', 'this', 'a', '2', 'star', 'but', 'I', 'think', 'it', 'only', 'deserves', '1', '.', 'I', 'simply', 'can', \"'\", 't', 'wrap', 'my', 'head', 'around', 'the', 'following', '.', 'picture', 'this', ',', 'you', 'add', 'a', 'task', 'to', 'your', 'list', '.', 'the', 'reminder', 'feature', 'is', 'on', '.', 'you', 'want', 'to', 'change', 'the', 'time', 'on', 'the', 'reminder', 'feature', 'so', 'you', 'click', 'it', '.']\n",
      "============================================================\n",
      "[2034, 1522, 1142, 170, 123, 2851, 1133, 146, 1341, 1122, 1178, 18641, 122, 119, 146, 2566, 1169, 112, 189, 10561, 1139, 1246, 1213, 1103, 1378, 119, 3439, 1142, 117, 1128, 5194, 170, 4579, 1106, 1240, 2190, 119, 1103, 15656, 2672, 1110, 1113, 119, 1128, 1328, 1106, 1849, 1103, 1159, 1113, 1103, 15656, 2672, 1177, 1128, 13440, 1122, 119]\n"
     ]
    }
   ],
   "source": [
    "#from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Set the model name\n",
    "MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "# Build a BERT based tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "sample_text = \"originally gave this a 2 star but I think it only deserves 1. I simply can't wrap my head around the following. picture this, you add a task to your list. the reminder feature is on.you want to change the time on the reminder feature so you click it.\"\n",
    "               \n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f'{sample_text}')\n",
    "print('='*60)\n",
    "print(tokens)\n",
    "print('='*60)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Frederico\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "      sample_text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=128,\n",
    "      truncation=True,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(reviews, all_ratings, train_size=0.8)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AmazonReview(Dataset):\n",
    "    def __init__(self, review, target, tokenizer, max_len):\n",
    "        self.review = review\n",
    "        self.target = target \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.review)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        review = self.review[index]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "                          review,\n",
    "                          add_special_tokens=True,\n",
    "                          max_length=128,\n",
    "                          truncation=True,\n",
    "                          return_token_type_ids=False,\n",
    "                          pad_to_max_length=True,\n",
    "                          return_attention_mask=True,\n",
    "                          return_tensors='pt')\n",
    "        \n",
    "        return {'review' : review,\n",
    "                'input_id': encoding['input_ids'].flatten(),\n",
    "                'attention_mask':encoding['attention_mask'].flatten(),\n",
    "                'target': torch.tensor(int(self.target[index]))\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "MAX_LEN = 128\n",
    "\n",
    "training_data = AmazonReview(review = np.array(X_train),\n",
    "                               target = np.array(y_train),\n",
    "                               tokenizer = tokenizer,\n",
    "                               max_len = MAX_LEN)\n",
    "\n",
    "validation_data = AmazonReview(review = np.array(X_valid),\n",
    "                               target = np.array(y_valid),\n",
    "                               tokenizer = tokenizer,\n",
    "                               max_len = MAX_LEN)\n",
    "\n",
    "test_data = AmazonReview(review = np.array(X_test),\n",
    "                               target = np.array(y_test),\n",
    "                               tokenizer = tokenizer,\n",
    "                               max_len = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 500 500\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data),len(test_data),len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "## DataLoader\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(training_data , batch_size = BATCH_SIZE , shuffle = True)\n",
    "test_loader = DataLoader(test_data , batch_size = BATCH_SIZE , shuffle = False)\n",
    "val_loader = DataLoader(validation_data , batch_size = BATCH_SIZE , shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16]) torch.Size([16, 128]) torch.Size([16, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Frederico\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sample_batch = iter(train_loader)\n",
    "sample_batch = next(sample_batch)\n",
    "sample_batch.keys()\n",
    "print(sample_batch['target'].shape, sample_batch['input_id'].shape, sample_batch['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last hidden layer size for input as batch torch.Size([16, 128, 768])\n",
      "Pooled output size for input as batch torch.Size([16, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "## testing the output of bert model\n",
    "x = bert_model(sample_batch['input_id'].view(BATCH_SIZE,MAX_LEN)\n",
    "               ,sample_batch['attention_mask'].view(BATCH_SIZE,MAX_LEN))\n",
    "\n",
    "print('Last hidden layer size for input as batch',x[0].shape)\n",
    "print('Pooled output size for input as batch',x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(SentimentClassifier,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(p = 0.3)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size,num_classes)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "    def forward(self,input_ids , attention_mask):\n",
    "        temp = self.bert(input_ids,attention_mask) # Here we have added one linear layer on top of \n",
    "        pooled_output = temp[1]                    # BERT-base with number of output = 3 \n",
    "        out = self.dropout(pooled_output)          # \n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5327, 0.4673],\n",
       "        [0.7064, 0.2936],\n",
       "        [0.4062, 0.5938],\n",
       "        [0.4981, 0.5019],\n",
       "        [0.6205, 0.3795],\n",
       "        [0.7215, 0.2785],\n",
       "        [0.5161, 0.4839],\n",
       "        [0.5461, 0.4539],\n",
       "        [0.4167, 0.5833],\n",
       "        [0.3698, 0.6302],\n",
       "        [0.7008, 0.2992],\n",
       "        [0.6439, 0.3561],\n",
       "        [0.4288, 0.5712],\n",
       "        [0.5901, 0.4099],\n",
       "        [0.6897, 0.3103],\n",
       "        [0.4832, 0.5168]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 2\n",
    "model = SentimentClassifier(num_classes)\n",
    "\n",
    "sample_linear_output = model(sample_batch['input_id']\n",
    "                             ,sample_batch['attention_mask'])\n",
    "\n",
    "s = nn.Softmax(dim=1)\n",
    "final_out = s(sample_linear_output)\n",
    "final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "## loss and optimizer\n",
    "learning_rate = 1e-5\n",
    "EPOCHS = 2\n",
    "\n",
    "criterian = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.AdamW(params = model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training the model given the model, dataset and weight directory where they are saved\n",
    "import time\n",
    "\n",
    "def train_model(model, trainloader, testloader, weights_dir, num_epochs=10):\n",
    "    start=time.time()\n",
    "    \n",
    "    \n",
    "    for epoch in range(0,num_epochs):\n",
    "\n",
    "        model.train()  # Put the network in train mode\n",
    "        #for i,d in len(trainloader),trainloader:\n",
    "        for i,d in enumerate(trainloader):\n",
    "\n",
    "            input_ids = d['input_id']\n",
    "            attention_masks = d['attention_mask']\n",
    "            targets = d['target']\n",
    "\n",
    "            optimizer.zero_grad()  # Set all currenly stored gradients to zero \n",
    "\n",
    "            y_pred = model(input_ids , attention_masks)\n",
    "\n",
    "            loss = criterian(y_pred, targets)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute relevant metrics\n",
    "\n",
    "            y_pred_max = torch.argmax(y_pred, dim=1)  # Get the labels with highest output probability\n",
    "\n",
    "            correct = torch.sum(torch.eq(y_pred_max, targets)).item()  # Count how many are equal to the true labels\n",
    "\n",
    "            elapsed = time.time() - start  # Keep track of how much time has elapsed\n",
    "\n",
    "            # Show progress every 20 batches \n",
    "            if not i % 5:\n",
    "                print(f'epoch: {epoch}, time: {elapsed:.3f}s, loss: {loss.item():.3f}, train accuracy: {correct / BATCH_SIZE:.3f}')\n",
    "\n",
    "        correct_total = 0\n",
    "\n",
    "        model.eval()  # Put the network in eval mode\n",
    "\n",
    "        eval_loss = []\n",
    "        correct_pred = 0 \n",
    "        \n",
    "        #for i, (x_batch, y_batch) in enumerate(val_loader):\n",
    "        for i,d in enumerate(val_loader):\n",
    "\n",
    "            input_ids = d['input_id']\n",
    "            attention_masks = d['attention_mask']\n",
    "            targets = d['target']\n",
    "\n",
    "\n",
    "            predictions = model(input_ids , attention_masks)\n",
    "            loss = criterian(predictions, targets)\n",
    "            # arg max acho eu\n",
    "            _,pred_classes = torch.max(predictions, dim=1)\n",
    "\n",
    "            eval_loss.append(loss.item())\n",
    "            correct_pred += torch.sum(pred_classes==targets)\n",
    "\n",
    "            correct_total += correct_pred\n",
    "        print(f'Accuracy on the test set: {correct_pred / len(validation_data):.3f}')\n",
    "\n",
    "        # Save weights every 10 epochs\n",
    "        if epoch%10==0:\n",
    "            torch.save(model.state_dict(), f\"./{weights_dir}/epoch-{epoch}_accuracy-{correct_total/len(validation_data):.3f}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# delete the directory of weights if it already exists and then create it\n",
    "if os.path.exists(os.path.join('weights3')):\n",
    "    shutil.rmtree(os.path.join('weights3'))\n",
    "!mkdir weights3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, time: 5.967s, loss: 0.722, train accuracy: 0.438\n",
      "epoch: 0, time: 35.623s, loss: 0.690, train accuracy: 0.625\n",
      "epoch: 0, time: 65.434s, loss: 0.729, train accuracy: 0.562\n",
      "epoch: 0, time: 94.841s, loss: 0.630, train accuracy: 0.562\n",
      "epoch: 0, time: 124.269s, loss: 0.610, train accuracy: 0.812\n",
      "epoch: 0, time: 153.525s, loss: 0.626, train accuracy: 0.688\n",
      "epoch: 0, time: 182.902s, loss: 0.572, train accuracy: 0.625\n",
      "epoch: 0, time: 212.329s, loss: 0.470, train accuracy: 0.812\n",
      "epoch: 0, time: 241.440s, loss: 0.642, train accuracy: 0.500\n",
      "epoch: 0, time: 270.512s, loss: 0.474, train accuracy: 0.812\n",
      "epoch: 0, time: 300.099s, loss: 0.540, train accuracy: 0.562\n",
      "epoch: 0, time: 328.935s, loss: 0.507, train accuracy: 0.750\n",
      "epoch: 0, time: 358.143s, loss: 0.652, train accuracy: 0.625\n",
      "epoch: 0, time: 387.337s, loss: 0.326, train accuracy: 0.875\n",
      "epoch: 0, time: 416.461s, loss: 0.449, train accuracy: 0.750\n",
      "epoch: 0, time: 445.630s, loss: 0.620, train accuracy: 0.500\n",
      "epoch: 0, time: 474.889s, loss: 0.366, train accuracy: 0.875\n",
      "epoch: 0, time: 504.255s, loss: 0.276, train accuracy: 0.938\n",
      "epoch: 0, time: 533.278s, loss: 0.323, train accuracy: 0.875\n",
      "epoch: 0, time: 562.196s, loss: 0.599, train accuracy: 0.750\n",
      "epoch: 0, time: 591.186s, loss: 0.350, train accuracy: 0.812\n",
      "epoch: 0, time: 620.413s, loss: 0.257, train accuracy: 0.812\n",
      "epoch: 0, time: 649.713s, loss: 0.233, train accuracy: 0.875\n",
      "epoch: 0, time: 678.660s, loss: 0.110, train accuracy: 1.000\n",
      "epoch: 0, time: 707.822s, loss: 0.357, train accuracy: 0.875\n",
      "epoch: 0, time: 736.922s, loss: 0.103, train accuracy: 1.000\n",
      "epoch: 0, time: 765.808s, loss: 0.279, train accuracy: 0.812\n",
      "epoch: 0, time: 794.938s, loss: 0.195, train accuracy: 0.938\n",
      "epoch: 0, time: 824.206s, loss: 0.386, train accuracy: 0.750\n",
      "epoch: 0, time: 852.924s, loss: 0.222, train accuracy: 0.938\n",
      "epoch: 0, time: 882.143s, loss: 0.188, train accuracy: 0.938\n",
      "epoch: 0, time: 911.046s, loss: 0.127, train accuracy: 1.000\n",
      "epoch: 0, time: 939.565s, loss: 0.637, train accuracy: 0.750\n",
      "epoch: 0, time: 968.320s, loss: 0.200, train accuracy: 0.938\n",
      "epoch: 0, time: 996.629s, loss: 0.120, train accuracy: 1.000\n",
      "epoch: 0, time: 1025.666s, loss: 0.274, train accuracy: 0.875\n",
      "epoch: 0, time: 1054.751s, loss: 0.360, train accuracy: 0.812\n",
      "epoch: 0, time: 1083.504s, loss: 0.295, train accuracy: 0.812\n",
      "epoch: 0, time: 1112.132s, loss: 0.164, train accuracy: 0.875\n",
      "epoch: 0, time: 1141.371s, loss: 0.172, train accuracy: 0.938\n",
      "epoch: 0, time: 1170.531s, loss: 0.508, train accuracy: 0.750\n",
      "epoch: 0, time: 1199.503s, loss: 0.197, train accuracy: 0.938\n",
      "epoch: 0, time: 1228.539s, loss: 0.189, train accuracy: 0.938\n",
      "epoch: 0, time: 1257.574s, loss: 0.305, train accuracy: 0.875\n",
      "epoch: 0, time: 1286.899s, loss: 0.168, train accuracy: 0.938\n",
      "epoch: 0, time: 1315.835s, loss: 0.309, train accuracy: 0.812\n",
      "epoch: 0, time: 1344.706s, loss: 0.277, train accuracy: 0.875\n",
      "epoch: 0, time: 1373.597s, loss: 0.249, train accuracy: 0.875\n",
      "epoch: 0, time: 1402.694s, loss: 0.217, train accuracy: 0.875\n",
      "epoch: 0, time: 1432.169s, loss: 0.344, train accuracy: 0.750\n",
      "Accuracy on the test set: 0.898\n",
      "epoch: 1, time: 1523.305s, loss: 0.147, train accuracy: 1.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Frederico\\Desktop\\cac\\CAC\\new_nlp_bert.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Frederico/Desktop/cac/CAC/new_nlp_bert.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_model(model, train_loader, val_loader, \u001b[39m'\u001b[39;49m\u001b[39mweights3\u001b[39;49m\u001b[39m'\u001b[39;49m, num_epochs\u001b[39m=\u001b[39;49mEPOCHS)\n",
      "\u001b[1;32mc:\\Users\\Frederico\\Desktop\\cac\\CAC\\new_nlp_bert.ipynb Cell 26\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, trainloader, testloader, weights_dir, num_epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Frederico/Desktop/cac/CAC/new_nlp_bert.ipynb#X34sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(input_ids , attention_masks)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Frederico/Desktop/cac/CAC/new_nlp_bert.ipynb#X34sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m criterian(y_pred, targets)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Frederico/Desktop/cac/CAC/new_nlp_bert.ipynb#X34sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Frederico/Desktop/cac/CAC/new_nlp_bert.ipynb#X34sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Frederico/Desktop/cac/CAC/new_nlp_bert.ipynb#X34sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Compute relevant metrics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Frederico\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Frederico\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, 'weights3', num_epochs=EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# function for loaduing weights of a trained model\n",
    "def load_weights(model, weights_dir):\n",
    "    files = os.listdir(weights_dir)\n",
    "    weight_paths = [os.path.join(weights_dir, basename) for basename in files]\n",
    "    # get the latest file in the directory\n",
    "    final_weight_file = os.path.basename(max(weight_paths, key=os.path.getctime))\n",
    "\n",
    "    # first model needs to be loaded\n",
    "    model = SentimentClassifier(num_classes)\n",
    "\n",
    "    # fixes odd error when state_dict has prescript \"model.\"\" in keys\n",
    "    state_dict = torch.load(os.path.join(weights_dir, final_weight_file))\n",
    "    for key in list(state_dict.keys()):\n",
    "        if key.startswith(\"model.\"):\n",
    "            state_dict[key[6:]] = state_dict.pop(key)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    print('Loaded weights: ' + final_weight_file)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights: epoch-0_accuracy-15.130.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentimentClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_weights(model, 'weights3')\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader):\n",
    "\n",
    "            input_ids = d['input_id']\n",
    "            attention_masks = d['attention_mask']\n",
    "            targets = d['target']\n",
    "\n",
    "\n",
    "\n",
    "            pred = model(input_ids , attention_masks)\n",
    "\n",
    "            pred_classes = torch.argmax(pred, dim=1)\n",
    "\n",
    "\n",
    "            predictions.extend(pred_classes)\n",
    "            prediction_probs.extend(pred_classes)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [01:03<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "y_pred, y_pred_probs, y_test = get_predictions(model,test_loader)\n",
    "y_pred = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1])\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.83       172\n",
      "           1       0.91      0.91      0.91       328\n",
      "\n",
      "    accuracy                           0.89       500\n",
      "   macro avg       0.87      0.87      0.87       500\n",
      "weighted avg       0.89      0.89      0.89       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class_names = ['0','1']\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[143  29]\n",
      " [ 28 300]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "    plt.ylabel('True sentiment')\n",
    "    plt.xlabel('Predicted sentiment');\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=128,\n",
    "      truncation=True,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt')\n",
    "    output = model(encoding['input_ids'],encoding['attention_mask'])\n",
    "    _,prediction = torch.max(output, dim=1)\n",
    "    return prediction.item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: I love completing my todos! Best app ever!!!\n",
      "Sentiment  : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Frederico\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "review_text = \"I love completing my todos! Best app ever!!!\"\n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Sentiment  : {predict(review_text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: Terrible, i hated it\n",
      "Sentiment  : 0\n"
     ]
    }
   ],
   "source": [
    "review_text = \"Terrible, i hated it\"\n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Sentiment  : {predict(review_text)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
